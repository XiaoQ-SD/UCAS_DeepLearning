{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision.transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from models.MNIST import MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# paramaters\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Datas ...\n",
      "Loading Datas Finished\n",
      "train datas of 469\n",
      "test datas of 79\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Datas ...\")\n",
    "train_dataset = datasets.MNIST(root='data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print(\"Loading Datas Finished\")\n",
    "\n",
    "print(\"train datas of %d\" % len(train_loader))\n",
    "print(\"test datas of %d\" % len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('data/MNIST/raw/train-images-idx3-ubyte', 'rb') as f:\n",
    "    file = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = [int(str(item).encode('ascii'), 16) for item in file[16: 16 + 784]]\n",
    "image_np = np.array(image, dtype=np.uint8).reshape(28, 28, 1)\n",
    "print(image_np.shape)\n",
    "cv2.imwrite('test.jpg', image_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNIST().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_index, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()  # 梯度初始化为0\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)  # 损失\n",
    "        pred = output.softmax(dim=1)  # 概率值的最大下标\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_index % 100 == 0:\n",
    "            print(\"train epoch %d, batch %d, loss %.6f\" % (epoch, batch_index, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, device, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0.0\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target).item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print(\"test average loss %.4f, accuracy %.4f\" % (test_loss, 100.0 * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 0, batch 0, loss 2.305865\n",
      "train epoch 0, batch 100, loss 0.054794\n",
      "train epoch 0, batch 200, loss 0.056644\n",
      "train epoch 0, batch 300, loss 0.107167\n",
      "train epoch 0, batch 400, loss 0.070186\n",
      "test average loss 0.0003, accuracy 98.6500\n",
      "train epoch 1, batch 0, loss 0.063086\n",
      "train epoch 1, batch 100, loss 0.083722\n",
      "train epoch 1, batch 200, loss 0.031873\n",
      "train epoch 1, batch 300, loss 0.032263\n",
      "train epoch 1, batch 400, loss 0.020221\n",
      "test average loss 0.0002, accuracy 99.1300\n",
      "train epoch 2, batch 0, loss 0.012650\n",
      "train epoch 2, batch 100, loss 0.005618\n",
      "train epoch 2, batch 200, loss 0.013819\n",
      "train epoch 2, batch 300, loss 0.006610\n",
      "train epoch 2, batch 400, loss 0.108925\n",
      "test average loss 0.0003, accuracy 98.9300\n",
      "train epoch 3, batch 0, loss 0.013500\n",
      "train epoch 3, batch 100, loss 0.004102\n",
      "train epoch 3, batch 200, loss 0.045152\n",
      "train epoch 3, batch 300, loss 0.038900\n",
      "train epoch 3, batch 400, loss 0.011184\n",
      "test average loss 0.0002, accuracy 99.0500\n",
      "train epoch 4, batch 0, loss 0.004579\n",
      "train epoch 4, batch 100, loss 0.008313\n",
      "train epoch 4, batch 200, loss 0.034037\n",
      "train epoch 4, batch 300, loss 0.028068\n",
      "train epoch 4, batch 400, loss 0.018240\n",
      "test average loss 0.0002, accuracy 99.1800\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, EPOCHS):\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch)\n",
    "    evaluate(model, DEVICE, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}